<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  
  

  


  
<!-- Mirrored from trac.mcs.anl.gov/projects/parallel-netcdf/wiki/CombiningOperations by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 29 Jan 2018 05:10:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <title>
      CombiningOperations – parallel-netcdf
    </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <!--[if IE]><script type="text/javascript">
      if (/^#__msie303:/.test(window.location.hash))
        window.location.replace(window.location.hash.replace(/^#__msie303:/, '#'));
    </script><![endif]-->
        <link rel="search" href="../search.html" />
        <link rel="help" href="TracGuide.html" />
        <link rel="alternate" href="./CombiningOperations?format=txt" type="text/x-trac-wiki" title="Plain Text" />
        <link rel="start" href="../wiki.html" />
        <link rel="stylesheet" href="../chrome/common/css/trac.css" type="text/css" /><link rel="stylesheet" href="../chrome/common/css/wiki.css" type="text/css" />
        <link rel="shortcut icon" href="../chrome/common/trac.ico" type="image/x-icon" />
        <link rel="icon" href="../chrome/common/trac.ico" type="image/x-icon" />
      <link type="application/opensearchdescription+xml" rel="search" href="../search/opensearch" title="Search parallel-netcdf" />
    <script type="text/javascript" src="../chrome/common/js/jquery.js"></script><script type="text/javascript" src="../chrome/common/js/babel.js"></script><script type="text/javascript" src="../chrome/common/js/messages/en_US.js"></script><script type="text/javascript" src="../chrome/common/js/trac.js"></script><script type="text/javascript" src="../chrome/common/js/search.js"></script><script type="text/javascript" src="../chrome/common/js/folding.js"></script>
    <!--[if lt IE 7]>
    <script type="text/javascript" src="/projects/parallel-netcdf/chrome/common/js/ie_pre7_hacks.js"></script>
    <![endif]-->
    <script type="text/javascript">
      jQuery(document).ready(function($) {
        $("#content").find("h1,h2,h3,h4,h5,h6").addAnchor(_("Link to this section"));
        $("#content").find(".wikianchor").each(function() {
          $(this).addAnchor(babel.format(_("Link to #%(id)s"), {id: $(this).attr('id')}));
        });
        $(".foldable").enableFolding(true, true);
      });
    </script>
  </head>
  <body>
    <div id="main">
        <div id="content" class="wiki">
      	  <div class="wikipage searchable">
        
          
          <div id="wikipage"><h1 id="IOaggregation:implicitmethodnonblockingAPIsandexplicitmethodmulti-variableAPIs">I/O aggregation: implicit method (nonblocking APIs) and explicit method (multi-variable APIs)</h1>
<hr />
<h2 id="Motivation">Motivation</h2>
<p>
Array variables comprise the bulk of the data in a netCDF dataset, and for
accesses to large regions of single array variables, PnetCDF attains very high
performance. However, most of the PnetCDF APIs that mimic netCDF's only allow access to one
array variable per call. If an application instead accesses a large number of
small-sized array variables, this interface limitation can cause significant
performance degradation, because high end network and storage systems deliver
much higher performance with larger request sizes. Moreover, the data for record
variables are stored interleaved by record, and the contiguity information
is lost.
</p>
<p>
We provided a new mechanism for PnetCDF to aggregate/combine multiple I/O requests for better I/O performance.
This mechanism is presented in two new sets of APIs: nonblocking APIs and multi-variable APIs.
The former is used to implicitly to aggregate requests by posting multiple requests, each can be
accessing to the same or a different variable, and then a single wait subroutine is used to combine
the requests and commit to the file system.
The latter is a new API family that takes arguments for reading/writing multiple array variables, allowing
application programmers to explicitly access multiple array variables in a single call.
Our performance evaluations, published in IASDS 2009 (reference below), demonstrate significant
improvement using well-known application benchmarks.
</p>
<h2 id="Usage">Usage</h2>
<p>
<strong>Nonblocking APIs</strong> (Implicit Method) -- The library accesses multiple variables implicitly.
Several variable accesses can be "scheduled" with the nonblocking routines.
Then, when the application waits for completion of those accesses, the library will service them all in a single call.
</p>
<p>
The term "nonblocking" here means the posting APIs are not blocked for waiting their I/O requests to complete.
PnetCDF simply registers the nonblocking requests internally and immediately return the control to user program.
There is no inter-process communication or I/O operation occurred in the posting call.
The request aggregation and real I/O operation is carried out at the wait subroutines.
</p>
<p>
In C:
</p>
<pre class="wiki">int
ncmpi_iget_vara_float(int               ncid,    /* in:  dataset ID */
                      int               varid,   /* in:  variable ID */
                      const MPI_Offset  start[], /* in:  [ndims] start offsets */
                      const MPI_Offset  count[], /* in:  [ndims] access counts */
                      float            *buf,     /* out: read buffer */
                      int              *req_id); /* out: request ID */

int
ncmpi_iput_vara_float(int               ncid,    /* in:  dataset ID */
                      int               varid,   /* in:  variable ID */
                      const MPI_Offset  start[], /* in:  [ndims] start offsets */
                      const MPI_Offset  count[], /* in:  [ndims] access counts */
                      const float      *buf,     /* in:  write buffer */
                      int              *req_id); /* out: request ID */

int
ncmpi_wait(int ncid,            /* in:  dataset ID */
           int num_reqs,        /* in:  number of requests */
           int req_ids[],       /* in:  [num_reqs] list of request IDs */
           int statuses[]);     /* out: [num_reqs] list of request statuses */

int
ncmpi_wait_all(int ncid,        /* in:  dataset ID */
               int num_reqs,    /* in:  number of requests */
               int req_ids[],   /* in:  [num_reqs] list of request IDs */
               int statuses[]); /* out: [num_reqs] list of request statuses */
</pre><p>
In Fortran:
</p>
<pre class="wiki">INTEGER FUNCTION nfmpi_iget_vara_real(ncid, varid, start, count, buf, req_id)
                            INTEGER,                       INTENT(IN)  :: ncid
                            INTEGER,                       INTENT(IN)  :: varid
                            INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN)  :: start(*)
                            INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN)  :: count(*)
                            REAL,                          INTENT(OUT) :: buf(*)
                            INTEGER,                       INTENT(OUT) :: req_id

INTEGER FUNCTION nfmpi_iput_vara_real(ncid, varid, start, count, buf, req_id)
                            INTEGER,                       INTENT(IN)  :: ncid
                            INTEGER,                       INTENT(IN)  :: varid
                            INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN)  :: start(*)
                            INTEGER(KIND=MPI_OFFSET_KIND), INTENT(IN)  :: count(*)
                            REAL,                          INTENT(IN)  :: buf(*)
                            INTEGER,                       INTENT(OUT) :: req_id

INTEGER FUNCTION nfmpi_wait(ncid, num_reqs, req_ids, statuses)
                            INTEGER,                       INTENT(IN)  :: ncid
                            INTEGER,                       INTENT(IN)  :: num_reqs
                            INTEGER,                       INTENT(IN)  :: req_ids(num_reqs)
                            INTEGER,                       INTENT(IN)  :: statuses(num_reqs)

INTEGER FUNCTION nfmpi_wait_all(ncid, num_reqs, req_ids, statuses)
                            INTEGER,                       INTENT(IN)  :: ncid
                            INTEGER,                       INTENT(IN)  :: num_reqs
                            INTEGER,                       INTENT(IN)  :: req_ids(num_reqs)
                            INTEGER,                       INTENT(IN)  :: statuses(num_reqs)
</pre><p>
The request posting calls, e.g. ncmpi_iput_vara_float() or nfmpi_iput_vara_real(), can be called either
collective or independent data mode.
In the 'wait' and 'wait_all' methods, the library actually batches up all
outstanding nonblocking operations.  In this way, we can carry out this
optimization without needing a thread. Note that 'ncmpi_wait_all()' must be called in
collective data mode, while 'ncmpi_wait()' in independent mode. 
</p>
<p>
<strong>Multi-variable APIs</strong> (Explicit Method) -- The caller explicitly accesses multiple variables at once.
New routines added to the library (e.g. ncmpi_mput_vara_all, indicating multiple puts) 
take a list of variables to access.  
</p>
<p>
The function calls for the explicit method look like this:
</p>
<pre class="wiki">int
ncmpi_mput_vara_all(int               ncid,         /* in:  dataset ID */
                    int               nvars,        /* in:  number of variables */
                    int               varids[],     /* in:  [nvars] list of variable IDs */
                    MPI_Offset* const starts[],     /* in:  [nvars][ndims] list of start offsets */
                    MPI_Offset* const counts[],     /* in:  [nvars][ndims] list of access counts */
                    void*       const bufs[],       /* in:  [nvars] list of buffer pointers */
                    MPI_Offset        bufcounts[],  /* in:  [nvars] list of buffer counts */
                    MPI_Datatype      datatypes[]); /* in:  [nvars] MPI derived datatype describing bufs */

int
ncmpi_mget_vara_all(int               ncid,         /* in:  dataset ID */
                    int               nvars,        /* in:  number of variables */
                    int               varids[],     /* in:  [nvars] list of variable IDs */
                    MPI_Offset* const starts[],     /* in:  [nvars][ndims] list of start offsets */
                    MPI_Offset* const counts[],     /* in:  [nvars][ndims] list of access counts */
                    void*             bufs[],       /* out: [nvars] list of buffer pointers */
                    MPI_Offset        bufcounts[],  /* in:  [nvars] list of buffer counts */
                    MPI_Datatype      datatypes[]); /* in:  [nvars] MPI derived datatype describing bufs */
</pre><p>
Do note that we do not have Fortran bindings for these new routines in
PnetCDF. Fortran codes should use the implicit method.
</p>
<h2 id="Exampleprograms">Example programs</h2>
<ul><li>Implicit Method
<ul><li>C:
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/C/nonblocking_write.c">nonblocking_write.c</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/C/block_cyclic.c">block_cyclic.c</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/C/column_wise.c">column_wise.c</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/tutorial/pnetcdf-write-nb.c">pnetcdf-write-nb.c</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/tutorial/pnetcdf-read-nb.c">pnetcdf-read-nb.c</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/tutorial/pnetcdf-write-buffered.c">pnetcdf-write-buffered.c</a>
</li><li>Fortran 77:
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F77/nonblocking_write.f">nonblocking_write.f</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F77/block_cyclic.f">block_cyclic.f</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F77/column_wise.f">column_wise.f</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/tutorial/pnetcdf-write-bufferedf77.f">pnetcdf-write-bufferedf77.f</a>
</li><li>Fortran 90:
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F90/nonblocking_write.f90">nonblocking_write.f90</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F90/block_cyclic.f90">block_cyclic.f90</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/F90/column_wise.f90">column_wise.f90</a>,
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/tutorial/pnetcdf-write-bufferedf.f90">pnetcdf-write-bufferedf.f90</a>
</li></ul></li></ul><ul><li>Explicit Method
<a class="source" href="https://github.com/Parallel-NetCDF/PnetCDF/blob/master/examples/C/mput.c">mput.c</a>
</li></ul><h2 id="References">References</h2>
<p>
We wrote a paper describing the new APIs, their implementation, and some results:
</p>
<p>
Kui Gao, Wei-keng Liao, Alok Choudhary, Robert Ross, and Robert Latham. "Combining I/O
Operations for Multiple Array Variables in Parallel NetCDF". In the <em>Proceedings
of  the <a class="ext-link" href="http://www.mcs.anl.gov/events/workshops/iasds09"><span class="icon">​</span>Workshop on Interfaces and Architectures for Scientific Data Storage</a></em>,
held in conjunction with the the IEEE Cluster Conference, New Orleans,
Louisiana, September 2009.  <a class="ext-link" href="http://www.mcs.anl.gov/~robl/pnetcdf/kgao:write-combining-iasds09.pdf"><span class="icon">​</span>PDF</a>
</p>
<p>
We used this new API to achieve better checkpoint and plotfile I/O in the FLASH astrophysics code:
Rob Latham, Chris Daley, Wei keng Liao, Kui Gao, Rob Ross, Anshu Dubey, and Alok Choudhary. "A case study for scientific I/O: improving the FLASH astrophysics code". <em>Computational Science &amp; Discovery</em>, 5(1):015001, 2012. <a class="ext-link" href="http://iopscience.iop.org/1749-4699/5/1/015001/"><span class="icon">​</span>online version</a>
</p>
</div>
        
        
      </div>
      

    </div>
    <div id="altlinks">
      <h3>Download in other formats:</h3>
      <ul>
        <li class="last first">
          <a rel="nofollow" href="./CombiningOperations?format=txt">Plain Text</a>
        </li>
      </ul>
    </div>
    </div>
    </body>

<!-- Mirrored from trac.mcs.anl.gov/projects/parallel-netcdf/wiki/CombiningOperations by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 29 Jan 2018 05:10:12 GMT -->
</html>