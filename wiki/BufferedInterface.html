<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title> PnetCDF Bufferred Nonblocking APIs </title>
<link rel="icon" href="../images/nuitfavicon.ico" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45938012-1', 'auto');
  ga('create', 'UA-45937491-1', 'auto', {'name': 'pnetcdfTracker'});
  ga('create', 'UA-46002884-1', 'auto', {'name': 'parallelnetcdfTracker'});
  ga('send', 'pageview'); 
  ga('pnetcdfTracker.send', 'pageview');
  ga('parallelnetcdfTracker.send', 'pageview');
</script>
</head>
<body>

<center> <h2>PnetCDF Bufferred Nonblocking APIs</h2> </center>
<hr>

We introduced a new class of API routines to the 1.3.0 release.  These
"buffered put APIs" (eg. ncmpi_bput_vara_float) make a copy of the user's
buffer internally, so the user's buffer can be reused when the call returns.
Their usage are similar to the iput APIs.  </p>
<p>
The usage of bput APIs is very similar to iput, except the followings.
</p>
<ol>
<li>users must tell PnetCDF the size of buffer to be used by PnetCDF internally
(attach and detach calls).  </li>
<li>once a bput API returns, user's buffer can be reused or freed (because the
write data has been copied to the internal buffer.) </li>
</ol>
<p>

The internal buffer is per file (as the attach API requires an ncid argument.)
It can be used to aggregate requests to multiple variables defined in the file.
</p>
<p>
The buffer will not be flushed automatically and all file I/O happens in
wait_all.  If the attached buffer ran out of space, NC_EINSUFFBUF error code
(non-fatal) will return.  It can be used to determine to call wait API, as
described above. However, an automatic flushing would require an MPI
independent I/O, again meaning a poor performance.  So, users are recommended
to make sure the buffer size is sufficient large.  In addition, if you rely on
PnetCDF to do type conversion between the I/O buffer and file data in two data
types of different size (e.g. I/O buffer is in short and NC file variable is in
4-byte int), you must calculate the size of attach buffer using the larger
type.  Once a call to wait/wait_all returns, the internal buffer usage is
reduced by those completely requests.
</p>
<p>
Here is an example code fragment for calling buffered APIs:
</p>
<pre>    ncmpi_buffer_attach(ncid, bufsize);
    ncmpi_bput_vara_float(ncid, varid0, start0, count0, buf0, &amp;req[0]);
    ncmpi_bput_vara_float(ncid, varid0, start1, count1, buf1, &amp;req[1]);
    ncmpi_bput_vara_int64(ncid, varid1, start2, count2, buf2, &amp;req[2]);
    ncmpi_wait_all(ncid, 3, req, status);
    ncmpi_buffer_detach(ncid);
</pre>

<h2>Check the current usage of the buffer</h2>
<p>
A new inquiry API is added to check the usage of the buffer. It can be useful
to know how much space is left for more buffered APIs calls.  This inquiry API
can be called in either collective or independent data mode.  </p>

<pre>
int ncmpi_inq_buffer_usage(int ncid, MPI_Offset *usage);
</pre>
<ul>
<li>"usage" will be returned with the current buffer usage in bytes.</li>
<li>Error codes may be invalid ncid (NC_EBADID) or no attached buffer found
(NC_ENULLABUF).  </li>
</ul>

<h2>Possible Future Directions</h2>
<p>
When the buffer fills up, the buffered APIs return NC_EINSUFFBUF and that
request is skipped.  It might be possible to implement automatic flushing of
the buffer, but as this process would be un-coordinated among processors, it
would have to be done independently and so would probably not perform very
well.  Perhaps there is a clever idea we have yet to come across.  </p>

<h2 id="Examples">Examples</h2>
<p>
Fortran 77: <a href=https://github.com/Parallel-NetCDF/PnetCDF/tree/master/examples/tutorial/pnetcdf-write-bufferedf77.f">pnetcdf-write-bufferedf77.f</a>
</p>
<p>
Fortran 90: <a href="https://github.com/Parallel-NetCDF/PnetCDF/tree/master/examples/tutorial/pnetcdf-write-bufferedf.f90">pnetcdf-write-bufferedf.f90</a>
</p>
<p>
C: <a href="https://github.com/Parallel-NetCDF/PnetCDF/tree/master/examples/tutorial/pnetcdf-write-buffered.c">pnetcdf-write-buffered.c</a>
</p>

<h2 id="Availability">Availability</h2>
<ul>
<li>The buffered interface was introduced in 1.3.0.  As this is a new interface
and still subject to experimentation, we recommend tracking PnetCDF's SVN trunk
for the latest implementation.  </li>
<li>The buffer usage inquiry API ncmpi_inq_buffer_usage/nfmpi_inq_buffer_usage
is added in 1.3.1 </li>
</ul>
<hr>
Return to <a href=https://parallel-netcdf.github.io>PnetCDF Home</a>
</body>
</html>
